{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98ea4fa7",
   "metadata": {},
   "source": [
    "### Diagram 1 â€” Quick Pipeline Overview\n",
    "*Two-cell structure with mermaid flowchart.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3216b09",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"ðŸ“‚ Raw Images\"] --> B[\"ðŸ§¹ Preprocess & Align\"]\n",
    "    B --> C[\"ðŸ§  Embedding Extraction\"]\n",
    "    C --> D[\"ðŸ‘¥ Attribute Models\"]\n",
    "    C --> E[\"ðŸ’š Attractiveness Model\"]\n",
    "    D --> F[\"ðŸ“Š Metadata Builder\"]\n",
    "    E --> F\n",
    "    F --> G[\"ðŸŽ¨ Analysis & Visualization\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fbe447",
   "metadata": {},
   "source": [
    "# FaceStats Tooling Snapshot\n",
    "\n",
    "Quick overview of the stack used across preprocessing, embeddings, attributes, and reporting.\n",
    "\n",
    "## Core Libraries\n",
    "- Python 3.x with Poetry/requirements.txt for dependency management\n",
    "- `torch` + `transformers` for CLIP/ViT embedding extraction and attribute models\n",
    "- `polars` and `numpy` for fast tabular/array ops; `sklearn` for PCA\n",
    "- `Pillow` (and optional `opencv-python`) for image I/O, resizing, and alignment helpers\n",
    "- `tqdm` for progress; `matplotlib`/`seaborn` for visualization; `nbformat` for notebook tweaks\n",
    "\n",
    "## Pipelines at a Glance\n",
    "- Preprocess: load -> normalize -> resize/alignment -> save to `data/preprocessed/`\n",
    "- Embeddings: CLIP/ViT forward pass -> L2 normalize -> `embeddings.parquet`\n",
    "- Attributes: HF image-classification pipelines -> `attributes.parquet`\n",
    "- Attractiveness: small MLP regressor on embeddings -> `scores.parquet`\n",
    "- Metadata: merge embeddings/attributes/scores -> `master.parquet`\n",
    "- Composites/Analysis: filter metadata, stack images, PCA/means, render composites and reports\n",
    "\n",
    "## Tool Map (Mermaid)\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Preprocess\n",
    "        P1[\"Pillow\"]\n",
    "        P2[\"OpenCV (optional)\"]\n",
    "        P3[\"Mediapipe FaceMesh\"]\n",
    "    end\n",
    "\n",
    "    subgraph Embeddings\n",
    "        E1[\"PyTorch\"]\n",
    "        E2[\"Transformers (CLIP/ViT)\"]\n",
    "    end\n",
    "\n",
    "    subgraph Attributes\n",
    "        A1[\"HF Pipelines\"]\n",
    "    end\n",
    "\n",
    "    subgraph Data\n",
    "        D1[\"Polars\"]\n",
    "        D2[\"NumPy\"]\n",
    "    end\n",
    "\n",
    "    subgraph Modeling\n",
    "        M1[\"PyTorch MLP\"]\n",
    "        M2[\"sklearn PCA\"]\n",
    "    end\n",
    "\n",
    "    subgraph Viz\n",
    "        V1[\"Matplotlib/Seaborn\"]\n",
    "    end\n",
    "\n",
    "    P1 & P2 & P3 --> E1\n",
    "    E1 --> E2\n",
    "    E2 --> A1\n",
    "    E2 --> M1\n",
    "    A1 --> D1\n",
    "    M1 --> D1\n",
    "    D1 --> M2\n",
    "    D1 --> V1\n",
    "```\n",
    "\n",
    "## Notes\n",
    "- Everything stays CPU-friendly unless you wire up GPU-backed PyTorch.\n",
    "- ONNX is not required; pure PyTorch + Transformers flows are the default.\n",
    "- Keep data paths consistent (`data/raw`, `data/preprocessed`, `embeddings.parquet`, `master.parquet`) to reuse the notebooks without edits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95192c4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
