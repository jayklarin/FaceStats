{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e27f30b",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "classDef notebook fill:#5c7fa6,stroke:#3f5a7b,color:#f2f6fb,font-weight:bold;\n",
    "classDef python fill:#9a80b8,stroke:#6d5789,color:#f7f3fb,font-weight:bold;\n",
    "classDef tools fill:#e9c48a,stroke:#b58950,color:#2d1c05;\n",
    "classDef methods fill:#8cc7ab,stroke:#5e9475,color:#0f2f1f;\n",
    "\n",
    "N02[\"02_embeddings.ipynb\"]:::notebook\n",
    "\n",
    "N02 --> E1[\"embed_clip.py\"]:::python\n",
    "\n",
    "E1 --> T2[\"Tools:<br>torch<br>transformers<br>Pillow<br>polars<br>numpy\"]:::tools\n",
    "\n",
    "T2 --> M2[\"Methods:<br>load_model()<br>extract_embedding()<br>run_embedding()<br>extract_clip_embeddings()<br>get_clip_embedding()\"]:::methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea2dab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /Users/jayklarin/__DI/Repositories/FaceStats\n",
      "src path added: /Users/jayklarin/__DI/Repositories/FaceStats/src\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "# Force notebook to run as if cwd = project root\n",
    "PROJECT_ROOT = \"/Users/jayklarin/__DI/Repositories/FaceStats\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(\"cwd:\", os.getcwd())\n",
    "\n",
    "# Add src/ to Python import path\n",
    "SRC_PATH = os.path.join(PROJECT_ROOT, \"src\")\n",
    "sys.path.insert(0, SRC_PATH)\n",
    "print(\"src path added:\", SRC_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffeebdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: mps\n",
      "Wrapped CLIPProcessor outputs to MPS.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using:\", device)\n",
    "\n",
    "processor = None  # We will re-bind it after construction\n",
    "\n",
    "class MPSProcessorWrapper:\n",
    "    \"\"\"\n",
    "    Wraps the processor *output only* so no recursion happens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, real_processor):\n",
    "        self.real = real_processor\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.real, name)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        out = self.real(*args, **kwargs)\n",
    "\n",
    "        # Move only tensors in the output batch to MPS\n",
    "        for k, v in out.items():\n",
    "            if torch.is_tensor(v):\n",
    "                out[k] = v.to(device)\n",
    "        return out\n",
    "\n",
    "# Build processor normally\n",
    "_real = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Wrap it safely (no recursion possible)\n",
    "processor = MPSProcessorWrapper(_real)\n",
    "\n",
    "print(\"Wrapped CLIPProcessor outputs to MPS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba62a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000 embeddings → data/processed/embeddings/embeddings_clip.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>filename</th><th>embedding</th></tr><tr><td>str</td><td>list[f64]</td></tr></thead><tbody><tr><td>&quot;SFHQ_pt4_00086092.jpg&quot;</td><td>[0.040853, -0.001176, … -0.034655]</td></tr><tr><td>&quot;SFHQ_pt4_00065309.jpg&quot;</td><td>[0.030933, 0.005503, … 0.001794]</td></tr><tr><td>&quot;SFHQ_pt4_00062466.jpg&quot;</td><td>[0.071319, -0.005249, … 0.051021]</td></tr><tr><td>&quot;SFHQ_pt4_00090828.jpg&quot;</td><td>[0.066199, -0.003542, … 0.020381]</td></tr><tr><td>&quot;SFHQ_pt4_00032251.jpg&quot;</td><td>[0.021252, -0.060645, … -0.004985]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "┌───────────────────────┬─────────────────────────────────┐\n",
       "│ filename              ┆ embedding                       │\n",
       "│ ---                   ┆ ---                             │\n",
       "│ str                   ┆ list[f64]                       │\n",
       "╞═══════════════════════╪═════════════════════════════════╡\n",
       "│ SFHQ_pt4_00086092.jpg ┆ [0.040853, -0.001176, … -0.034… │\n",
       "│ SFHQ_pt4_00065309.jpg ┆ [0.030933, 0.005503, … 0.00179… │\n",
       "│ SFHQ_pt4_00062466.jpg ┆ [0.071319, -0.005249, … 0.0510… │\n",
       "│ SFHQ_pt4_00090828.jpg ┆ [0.066199, -0.003542, … 0.0203… │\n",
       "│ SFHQ_pt4_00032251.jpg ┆ [0.021252, -0.060645, … -0.004… │\n",
       "└───────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from embeddings.embed_clip import extract_clip_embeddings\n",
    "\n",
    "INPUT_DIR = \"data/processed/preproc\"\n",
    "OUTPUT_FILE = \"data/processed/embeddings/embeddings_clip.parquet\"\n",
    "\n",
    "emb_df = extract_clip_embeddings(\n",
    "    input_dir=INPUT_DIR,\n",
    "    output_path=OUTPUT_FILE,\n",
    "    model_name=\"openai/clip-vit-base-patch32\"\n",
    ")\n",
    "\n",
    "emb_df.head()\n",
    "# 35 minutes for 10,000 images on cpu without mps acceleration\n",
    "# About 25 minutes on mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9336518d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facestats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
